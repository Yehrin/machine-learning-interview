# 题目列表

**一、机器学习相关**

**1、 基本概念**

- [ ] [1-1  简述解决一个机器学习问题时，你的流程是怎样的？](#1-1)

- [ ] [1-2  损失函数是什么，如何定义合理的损失函数？](#1-2)

- [ ] [1-3  回归模型和分类模型常用损失函数有哪些？各有什么优缺点](#1-3)

- [ ] [1-4  什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？](#1-4)

- [ ] [1-5  模型的“泛化”能力是指？如何提升模型泛化能力？](#1-5)

- [ ] [1-6  如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？](#1-6)

- [ ] [1-7  什么是混淆矩阵？](#1-7)

- [ ] [1-8  ROC曲线如何绘制？相比P-R曲线有什么特点？](#1-8)

- [x] [1-9  如何评判模型是过拟合还是欠拟合？遇到过拟合或欠拟合时，你是如何解决？](#1-9)

- [ ] [1-10  你是如何针对应用场景选择合适的模型？](#1-10)

- [ ] [1-11  如何选择模型中的超参数？有什么方法，并说说其优劣点](#1-11)

- [ ] [1-12  误差分析是什么？你是如何进行误差分析？](#1-12)

- [ ] [1-13  你是如何理解模型的偏差和方差？什么样的情况是高偏差，什么情况是高方差？](#1-13)

- [ ] [1-14  出现高偏差或者高方差的时候你有什么优化策略？](#1-14)

- [ ] [1-15  奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？举例说明](#1-15)

- [ ] [1-16  线性模型和非线性模型的区别？哪些模型是线性模型，哪些模型是非线性模型？](#1-16)

- [ ] [1-17  生成式模型和判别式模型的区别？哪些模型是生成式模型，哪些模型是判别式模型？](#1-17)



**2、经典机器学习**

**特征工程**

- [ ] [2-1-1  你是怎样理解“特征”？](#2-1-1)

- [ ] [2-1-2  给定场景和问题，你如何设计特征？（特征工程方法论）](#2-1-2)

- [ ] [2-1-3  开发特征时候做如何做数据探索，怎样选择有用的特征？](#2-1-3)

- [ ] [2-1-4  你是如何做数据清洗的？举例说明](#2-1-4)

- [ ] [2-1-5  如何发现数据中的异常值，你是如何处理？](#2-1-5)

- [ ] [2-1-6  缺失值如何处理？](#2-1-6)

- [ ] [2-1-7  对于数值类型数据，你会怎样处理？为什么要做归一化？归一化有哪些方法？离散些方法，离散化和归一化有哪些优缺点？](#2-1-7)

- [ ] [2-1-8  标准化和归一化异同？](#2-1-8)

- [ ] [2-1-9  你是如何处理CTR类特征？](#2-1-9)

- [ ] [2-1-10  讲解贝叶斯平滑原理？以及如何训练得到平滑参数](#2-1-10)

- [ ] [2-1-11  类别型数据你是如何处理的？比如游戏品类，地域，设备](#2-1-11)

- [ ] [2-1-12  序号编码、one-hot编码、二进制编码都是什么？适合怎样的类别型数据？](#2-1-12)

- [ ] [2-1-13  时间类型数据你的处理方法是什么？原因？](#2-1-13)

- [ ] [2-1-14  你怎样理解组合特征？举个例子，并说明它和单特征有啥区别](#2-1-14)

- [ ] [2-1-15  如何处理高维组合特征？比如用户ID和内容ID？](#2-1-15)

- [ ] [2-1-16  如何理解笛卡尔积、外积、内积？](#2-1-16)

- [ ] [2-1-17  文本数据你会如何处理？](#2-1-17)

- [ ] [2-1-18  文本特征表示有哪些模型？他们的优缺点都是什么？](#2-1-18)

- [ ] [2-1-19  讲解TFF原理，它有什么优点和缺点？针对它的缺点，你有什么优化思路？](#2-1-19)

- [ ] [2-1-20  N-gram算法是什么？有什么优缺点？](#2-1-20)

- [ ] [2-1-21  讲解一下word2vec工作原理？损失函数是什么？](#2-1-21)

- [ ] [2-1-22  讲解一下LDA模型原理和训练过程？](#2-1-22)

- [ ] [2-1-23  Word2vec和LDA两个模型有什么区别和联系？](#2-1-23)

- [ ] [2-1-24  Skin-gram和cbow有何异同？](#2-1-24)

- [ ] [2-1-25  图像数据如何处理？有哪些常用的图像特征提取方法](#2-1-25)

- [ ] [2-1-26  你是怎样做特征选择的？卡方检验、信息值（IV）、VOE都是如何计算？](#2-1-26)

- [ ] [2-1-27  计算特征之间的相关性方法有哪些？有什么优缺点](#2-1-27)



**基础算法原理和推导**

**KNN**

- [ ] [2-2-1  Knn建模流程是怎样的？](#2-2-1)

- [ ] [2-2-2  Knn优缺点是什么？](#2-2-2)

- [ ] [2-2-3  Knn适合什么样的场景和数据类型？](#2-2-3)

- [ ] [2-2-4  常用的距离衡量公式都有哪些？具体说明它们的计算流程，以及使用场景？](#2-2-4)

- [ ] [2-2-5  超参数K值过大或者过小对结果有什么影响，你是如何选择K值？](#2-2-5)

- [ ] [2-2-6  介绍一下Kd树？如何建树，以及如何搜索最近节点？](#2-2-6)

  ·

**支持向量机**

- [ ] [2-3-1  简单讲解SVM模型原理？](#2-3-1)

- [ ] [2-3-2  SVM为什么会对缺失值敏感？实际应用时候你是如何处理？](#2-3-2)

- [ ] [2-3-3  SVM为什么可以分类非线性问题？](#2-3-3)

- [ ] [2-3-4  常用的核函数有哪些？你是如何选择不同的核函数的？](#2-3-4)

- [ ] [2-3-5  RBF核函数一定线性可分么？为什么](#2-3-5)

- [ ] [2-3-6  SVM属于线性模型还是非线性模型？为什么？](#2-3-6)

- [ ] [2-3-7  训练误差为0的SVM分类器一定存在吗？说明原因？](#2-3-7)



**朴素贝叶斯模型**

- [ ] [2-4-1  讲解贝叶斯定理？](2-4-1)

- [ ] [2-4-2  什么是条件概率、边缘概率、联合概率？](#2-4-2)

- [ ] [2-4-3  后验概率最大化的含义是什么？](#2-4-3)

- [ ] [2-4-4  朴素贝叶斯模型如何学习的？训练过程是怎样？](#2-4-4)

- [ ] [2-4-5  你如何理解生成模型和判别模型？](#2-4-5)

- [ ] [2-4-6  朴素贝叶斯模型“朴素”体现在哪里？存在什么问题？有哪些优化方向？](#2-4-6)

- [ ] [2-4-7  什么是贝叶斯网络？它能解决什么问题？](#2-4-7)

- [ ] [2-4-8  为什么说朴素贝叶斯也是线性模型而不是非线性模型呢？](#2-4-8)



**线性回归**

- [ ] [2-5-1  线性回归的基本思想是？](#2-5-1)

- [ ] [2-5-2  什么是“广义线性模型”？](#2-5-2)

- [ ] [2-5-3  线性回归常用的损失函数有哪些？优化算法有哪些？](#2-5-3)

- [ ] [2-5-4  线性回归适用什么类型的问题？有哪些优缺点？](#2-5-4)

- [ ] [2-5-5  请用最小二乘法推倒参数更新公式？](#2-5-5)



**逻辑回归**

- [ ] [2-6-1  逻辑回归相比于线性回归有什么异同？](#2-6-1)

- [ ] [2-6-2  逻辑回归和广义线性模型有何关系？](#2-6-2)

- [ ] [2-6-3  逻辑回归如何处理多标签分类？](#2-6-3)

- [ ] [2-6-4  为什么逻辑回归需要进行归一化或者取对数？](#2-6-4)

- [ ] [2-6-5  为什么逻辑回归把特征离散化之后效果会提升？](#2-6-5)

- [ ] [2-6-6  类别不平衡问题你是如何处理的？什么是过采样，什么是欠采样？举例](#2-6-6)

- [ ] [2-6-7  讲解L1和L2正则，它们都有什么作用，解释为什么L1比L2更容易产生稀疏解；对于存在线性相关的一组特征，L1正则如何选择特征？](#2-6-7)

- [ ] [2-6-8  使用交叉熵作为损失函数，梯度下降作为优化方法，推倒参数更新公式](#2-6-8)

- [ ] [2-6-9  代码写出训练函数](#2-6-9)



**FM模型**

- [ ] [2-7-1  FM模型与逻辑回归相比有什么优缺点？](#2-7-1)

- [ ] [2-7-2  为什么FM模型计算复杂度时O(kn)？](#2-7-2)

- [ ] [2-7-3  介绍FFM场感知分解机器（Field-aware Factorization Machine），说说与FM异同？](#2-7-3)

- [ ] [2-7-4  使用FM进行模型训练时候，有哪些核心参数对模型效果影响大？](#2-7-4)

- [ ] [2-7-5  如何从神经网络的视角看待FM模型？](#2-7-5)



**决策树**

- [x] [2-8-1  讲解完成的决策树的建树过程](#2-8-1)

- [x] [2-8-2  你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？](#2-8-2)

- [ ] [2-8-3  联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？](#2-8-3)

- [x] [2-8-4  常用的决策树有哪些？ID3、C4.5、CART有啥异同？](#2-8-4)

- [ ] [2-8-5  决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么](#2-8-5)



**随机森林（RF）**

- [ ] [2-9-1  介绍RF原理和思想](#2-9-1)

- [ ] [2-9-2  RF是如何处理缺失值？](#2-9-2)

- [ ] [2-9-3  RF如何衡量特征重要度？](#2-9-3)

- [ ] [2-9-4  RF“随机”主要体现在哪里？](#2-9-4)

- [ ] [2-9-5  RF有哪些优点和局限性？](#2-9-5)

- [ ] [2-9-6  为什么多个弱分类器组合效果会比单个要好？如何组合弱分类器可以获得更好的结果？原因是什么？](#2-9-6)

- [ ] [2-9-7  Bagging的思想是什么？它是降低偏差还是方差，为什么？](#2-9-7)

- [ ] [2-9-8  可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？](#2-9-8)



**GBDT**

- [ ] [2-10-1  梯度提升和梯度下降有什么区别和联系？](#2-10-1)

- [ ] [2-10-2  你是如何理解Boosting和Bagging？他们有什么异同？](#2-10-2)

- [ ] [2-10-3  讲解GBDT的训练过程？](#2-10-3)

- [ ] [2-10-4  你觉得GBDT训练过程中哪些环节可以平行提升训练效率？](#2-10-4)

- [ ] [2-10-5  GBDT的优点和局限性有哪些？](#2-10-5)

- [ ] [2-10-6  GBDT是否对异常值敏感，为什么？](#2-10-6)

- [ ] [2-10-7  如何防止GBDT过拟合？](#2-10-7)

- [ ] [2-10-8  在训练过程中哪些参数对模型效果影响比较大？这些参数造成影响是什么？](#2-10-8)



**k-means**

- [ ] [2-11-1  简述kmeans建模过程？](#2-11-1)

- [ ] [2-11-2  Kmeans损失函数是如何定义？](#2-11-2)

- [ ] [2-11-3  你是如何选择初始类族的中心点？](#2-11-3)

- [ ] [2-11-4  如何提升kmeans效率？](#2-11-4)

- [ ] [2-11-5  常用的距离衡量方法有哪些？他们都适用什么类型问题？](#2-11-5)

- [ ] [2-11-6  Kmeans对异常值是否敏感？为什么？](#2-11-6)

- [ ] [2-11-7  如何评估聚类效果？](#2-11-7)

- [ ] [2-11-8  超参数类的个数k如何选取？](#2-11-8)

- [ ] [2-11-9  Kmeans有哪些优缺点？是否有了解过改进的模型，举例说明？](#2-11-9)

- [ ] [2-11-10  试试证明kmeans算法的收敛性](#2-11-10)

- [ ] [2-11-11  除了kmeans聚类算法之外，你还了解哪些聚类算法？简要说明原理](#2-11-11)



**PCA降维**

- [ ] [2-12-1  为什么要对数据进行降维？它能解决什么问题？](#2-12-1)

- [ ] [2-12-2  你是如何理解维度灾难？](#2-12-1)

- [ ] [2-12-3  PCA主成分分析思想是什么？](#2-12-1)

- [ ] [2-12-4  如何定义主成分？](#2-12-1)

- [ ] [2-12-5  如何设计目标函数使得降维达到提取主成分的目的？](#2-12-1)

- [ ] [2-12-6  PCA有哪些局限性？如何优化](#2-12-1)

- [ ] [2-12-7  线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？](#2-12-1)



**3、 深度学习**

**DNN**

- [ ] [3-1-1  描述一下神经网络？推倒反向传播公式？](#3-1-1)

- [x] [3-1-2  讲解一下dropout原理？](#3-1-2)

- [ ] [3-1-3  梯度消失和梯度膨胀的原因是什么？有什么方法可以缓解？](#3-1-3)

- [ ] [3-1-4  什么时候该用浅层神经网络，什么时候该选择深层网络](#3-1-4)

- [ ] [3-1-5  Sigmoid、Relu、Tanh激活函数都有哪些优缺点？](#3-1-5)

- [ ] [3-1-6  写出常用激活函数的导数](#3-1-6)

- [ ] [3-1-7  训练模型的时候，是否可以把网络参数全部初始化为0？为什么](#3-1-7)

- [ ] [3-1-8  Batchsize大小会如何影响收敛速度？](#3-1-8)



3-1-1



**CNN**

- [ ] [3-2-1  简述CNN的工作原理？](#3-2-1)

- [ ] [3-2-2  卷积核是什么？选择大卷积核和小卷积核有什么影响？](#3-2-2)

- [ ] [3-2-3  你在实际应用中如何设计卷积核？](#3-2-3)

- [ ] [3-2-4  为什么CNN具有平移不变性？](#3-2-4)

- [ ] [3-2-5  Pooling操作是什么？有几种？作用是什么？](#3-2-5)

- [ ] [3-2-6  为什么CNN需要pooling操作？](#3-2-6)

- [ ] [3-2-7  什么是batchnormalization？它的原理是什么？在CNN中如何使用？](#3-2-7)

- [ ] [3-2-8  卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性以其作用？](#3-2-8)

- [ ] [3-2-9  你是如何理解fine-tune？有什么技巧](#3-2-9)

 

**RNN**

- [ ] [3-3-1  简述RNN模型原理，说说RNN适合解决什么类型问题？为什么](#3-3-1)

- [ ] [3-3-2  RNN和DNN有何异同？](#3-3-2)

- [ ] [3-3-3  RNN为什么有记忆功能？](#3-3-3)

- [ ] [3-3-4  长短期记忆网络LSTM是如何实现长短期记忆功能的？](#3-3-4)

- [ ] [3-3-5  长短期记忆网络LSTM各模块都使用什么激活函数，可以使用其他激活函数么？](#3-3-5)

- [ ] [3-3-6  GRU和LSTM有何异同](#3-3-6)

- [ ] [3-3-7  什么是Seq2Seq模型？该模型能解决什么类型问题？](#3-3-7)

- [ ] [3-3-8  注意力机制是什么？Seq2Seq模型引入注意力机制主要解决什么问题？](#3-3-8)


**4、 基础工具**

**Spark**

- [ ] [4-1-1  什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？](#4-1-1)

- [ ] [4-1-2  Transformation和action算子有什么区别？举例说明](#4-1-2)

- [ ] [4-1-3  讲解sparkshuffle原理和特性？shuffle write 和 huffleread过程做些什么？](#4-1-1)

- [ ] [4-1-4  哪些spark算子会有shuffle？](#4-1-1)

- [ ] [4-1-5  讲解sparkschedule（任务调度）？](#4-1-1)

- [ ] [4-1-6  Sparkstage是如何划分的？](#4-1-1)

- [ ] [4-1-7  Sparkcache一定能提升计算性能么？说明原因？](#4-1-1)

- [ ] [4-1-8  Cache和persist有什么区别和联系？](#4-1-1)

- [ ] [4-1-9  RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？](#4-1-1)

- [ ] [4-1-10  当GC时间占比很大可能的原因有哪些？对应的优化方法是？](#4-1-1)

- [ ] [4-1-11  park中repartition和coalesce异同？coalesce什么时候效果更高，为什么](#4-1-1)

- [ ] [4-1-12  Groupbykey和reducebykey哪个性能更高，为什么？](#4-1-1)

- [ ] [4-1-13  你是如何理解caseclass的？](#4-1-1)

- [ ] [4-1-14  Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候用class](#4-1-1)

- [ ] [4-1-15  Scala 语法中to 和 until有啥区别](#4-1-1)

- [ ] [4-1-16  讲解Scala伴生对象和伴生类](#4-1-1)





**Xgboost**

- [ ] [4-2-1  你选择使用xgboost的原因是什么？](#4-2-1)

- [ ] [4-2-2  Xgboost和GBDT有什么异同？](#4-2-2)

- [ ] [4-2-3  为什么xgboost训练会那么快，主要优化点事什么？](#4-2-3)

- [ ] [4-2-4  Xgboost是如何处理缺失值的？](#4-2-4)

- [ ] [4-2-5  Xgboost和lightGBM有哪些异同？](#4-2-5)

- [ ] [4-2-6  Xgboost为什么要使用泰勒展开式，解决什么问题？](#4-2-6)

- [ ] [4-2-7  Xgboost是如何寻找最优特征的？](#4-2-7)



**Tensorflow**

- [ ] [4-3-1  使用tensorflow实现逻辑回归，并介绍其计算图](#4-3-1)

- [ ] [4-3-2  sparse_softmax_cross_entropy_with_logits和softmax_cross_entropy_with_logits有何异同？](#4-3-2)

- [ ] [4-3-3  使用tensorflow过程中，常见调试哪些参数？举例说明](#4-3-3)

- [ ] [4-3-4  Tensorflow梯度更新是同步还是异步，有什么好处？](#4-3-4)

- [ ] [4-3-5  讲解一下TFRecords](#4-3-5)

- [ ] [4-3-6  tensorflow如何使用如何实现超大文件训练？](#4-3-6)

- [ ] [4-3-7  如何读取或者加载图片数据？](#4-3-7)



**5、推荐系统**

- [ ] [5-1-1  你是如何选择正负样本？如何处理样本不均衡的情况？](#5-1-1)

- [ ] [5-1-2  如何设计推荐场景的特征体系？举例说明](#5-1-1)

- [ ] [5-1-3  你是如何建立用户模型来理解用户，获取用户兴趣的？](#5-1-1)

- [ ] [5-1-4  你是如何选择适合该场景的推荐模型？讲讲你的思考过程](#5-1-1)

- [ ] [5-1-5  你是如何理解当前流行的召回->粗排->精排的推荐架构？这种架构有什么优缺点？什么场景适用使用，什么场景不适合？](#5-1-1)

- [ ] [5-1-6  如何解决热度穿透的问题？（因为item热度非常高，导致ctr类特征主导排序，缺少个性化的情况）](#5-1-1)

- [ ] [5-1-7  用户冷启动你是如何处理的？](#5-1-1)
- [ ] [5-1-8  新内容你是如何处理的？](#5-1-1)

- [ ] [5-1-9  你们使用的召回算法有哪些？如何能保证足够的召回率？](#5-1-1)

- [ ] [5-1-10  实时数据和离线数据如何融合？工程上是怎样实现？如何避免实时数据置信度不高带来的偏差问题？](#5-1-1)

- [ ] [5-1-11  你们是如何平衡不同优化目标的问题？比如：时长、互动等](#5-1-1)

- [ ] [5-1-12  不同类型内容推荐时候，如何平衡不同类型内容，比如图文、视频；或者不同分类](#5-1-1)

- [ ] [5-1-13  如何保证线上线下数据一致性？工程上是如何实现？](#5-1-1)

- [ ] [5-1-14  离线训练效果好，但是上线效果不明显或在变差可能是什么问题？如何解决？](#5-1-1)

- [ ] [5-1-15  在实际业务中，出现badcase,你是如何快速反查问题的？举例说明](#5-1-1)

- [ ] [5-1-16  使用ctr预估的方式来做精排，会不会出现相似内容大量聚集？原因是什么？你是如何解决的？](#5-1-1)

- [ ] [5-1-17  你了解有多少种相关推荐算法？各有什么优缺点](#5-1-1)

- [ ] [5-1-18  深度学习可以应用到推荐问题上解决哪些问题？为什么比传统机器学习要好？](#5-1-1)



**二、数学相关**

**6、 概率论和统计学**

- [ ] [6-1-1  说说你是怎样理解信息熵的？](#6-1-1)

- [ ] [6-1-2   能否从数据原理熵解析信息熵可以表示随机变量的不确定性？](#6-1-2)

- [ ] [6-1-3  怎样的模型是最大熵模型？它有什么优点](#6-1-3)

- [ ] [6-1-4  什么是Beta分布？它与二项分布有什么关系？](#6-1-4)

- [ ] [6-1-5   什么是泊松分布？它与二项分布有什么关系？](#6-1-5)

- [ ] [6-1-6  什么是t分布？他与正态分布有什么关系？](#6-1-6)

- [ ] [6-1-7    什么是多项式分布？具体说明？](#6-1-7)

- [ ] [6-1-8   参数估计有哪些方法？](#6-1-8)

- [ ] [6-1-9  点估计和区间估计都是什么？](#6-1-9)

- [ ] [6-1-10  讲解一下极大似然估计，以及适用场景？](#6-1-10)



7、 最优化问题

- [ ] [7-1-1  什么是梯度？](#7-1-1)

- [ ] [7-1-2  梯度下降找到的一定是下降最快的方法？](#7-1-1)

- [ ] [7-1-3  牛顿法和梯度法有什么区别？](#7-1-1)

- [ ] [7-1-4  什么是拟牛顿法？](#7-1-1)

- [ ] [7-1-5  讲解什么是拉格朗日乘子法、对偶问题、kkt条件?](#7-1-1)

- [ ] [7-1-6  是否所有的优化问题都可以转化为对偶问题？](#7-1-1)

- [ ] [7-1-7  讲解SMO（SequentialMinimalOptimization）算法基本思想？](#7-1-1)

- [ ] [7-1-8  为什么深度学习不用二阶优化？](#7-1-1)

- [ ] [7-1-9  讲解SGD，ftrl、Adagrad，Adadelta，Adam，Adamax，Nadam优化算法以及他们的联系和优缺点](#7-1-1)

- [ ] [7-1-10 为什么batch size大，训练速度快or为什么mini-batch比SGD快？](#7-1-10)

- [ ] [7-1-11  为什么不把batch size设得特别大](#7-11)









# 解答

# **一、机器学习相关**

## **1、 基本概念**

- [ ] [1-1  简述解决一个机器学习问题时，你的流程是怎样的？]()

- [ ] [1-2  损失函数是什么，如何定义合理的损失函数？](#1-2)

  机器学习模型关于单个样本的预测值与真实值的差称为**损失**。用于计算损失的函数称为**损失函数**。

- [ ] [1-3  回归模型和分类模型常用损失函数有哪些？各有什么优缺点](#1-3)

- [ ] [1-4  什么是结构误差和经验误差？训练模型的时候如何判断已经达到最优？](#1-4)

- [ ] [1-5  模型的“泛化”能力是指？如何提升模型泛化能力？](#1-5)

- [ ] [1-6  如何选择合适的模型评估指标？AUC、精准度、召回率、F1值都是什么？如何计算？有什么优缺点？](#1-6)

**Accuracy**（准确率）：分类正确的样本占总样本个数的比例
$$
Accuracy = \frac{n_{correct}}{n_{total}}
$$
​	缺点：不同类别的样本比例非常不均衡时，占比达的类别往往成为影响准确率的最主要因素。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。

​	解决：可以使用每个类别下的样本准确率的算术平均（平均准确率）作为模型评估的指标。

**Precision**（精确率）：分类正确的正样本个数占分类器判定为正样本的样本个数的比例

**Recall**（召回率）：分类正确的正样本数占真正的正样本个数的比例

**F1-score**：precision和recall的调和平均值
$$
{\rm F1} = \frac{2 \times precision \times recall}{precision + recall}
$$
在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用Top N返回结果的Precision和Recall值来衡量排序模型的性能。即认为模型返回的Top N结果就是模型判定的正样本，计算前N个位置的Precision@N和Recall@N。为了综合评估一个排序模型的好坏，不仅要看模型在不同Top N下的Precision@N和Recall@N，而且最好画出模型的P-R曲线。P-R曲线的横轴是Recall，纵轴是Precision。

**AUC**：ROC曲线下的面积大小。计算AUC值只要沿着ROC横轴做积分就可以。

l  什么是混淆矩阵？

> l  ROC曲线如何绘制？相比P-R曲线有什么特点？

**ROC**：横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）
$$
FPR = \frac{FP}{N} \\
TPR = \frac{TP}{P}
$$
其中P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被预测为正样本的个数。

【如何绘制ROC曲线】通过不断移动分类器的“截断点”来生成曲线上的一组关键点。在二分类问题中，模型输出一般是预测样本为正例的概率，在输出最终的正例负例之前，我们需要制定一个阈值。大于该阈值的样本判定为正例，小于该阈值的样本判定为负例。通过动态调整截断点，绘制每个截断点对应位置，再连接所有点得到最终的ROC曲线。





- [ ] [1-7  什么是混淆矩阵？](#1-7)
- [ ] [1-8  ROC曲线如何绘制？相比P-R曲线有什么特点？](#1-8)
- [ ] [1-9  如何评判模型是过拟合还是欠拟合？遇到过拟合或欠拟合时，你是如何解决？](#1-9)

当训练集效果差，欠拟合（如accuracy<0.8）；训练集效果好，测试集效果差，过拟合

欠拟合解决方法：

（1）增加每层网络的神经元数、增加层数

过拟合解决方法：

（1）Early Stopping

（2）正则项L1 L2

（3）Dropout



- [ ] [1-10  你是如何针对应用场景选择合适的模型？](#1-10)
- [ ] [1-11  如何选择模型中的超参数？有什么方法，并说说其优劣点](#1-11)
- [ ] [1-12  误差分析是什么？你是如何进行误差分析？](#1-12)
- [ ] [1-13  你是如何理解模型的偏差和方差？什么样的情况是高偏差，什么情况是高方差？](#1-13)
- [ ] [1-14  出现高偏差或者高方差的时候你有什么优化策略？](#1-14)
- [ ] [1-15  奥卡姆剃刀定律是什么？对机器学习模型优化有何启发？举例说明](#1-15)
- [ ] [1-16  线性模型和非线性模型的区别？哪些模型是线性模型，哪些模型是非线性模型？](#1-16)
- [ ] [1-17  生成式模型和判别式模型的区别？哪些模型是生成式模型，哪些模型是判别式模型？](#1-17)



## **2、经典机器学习**

### **特征工程**

- [ ] [2-1-1  你是怎样理解“特征”？](#2-1-1)
- [ ] [2-1-2  给定场景和问题，你如何设计特征？（特征工程方法论）](#2-1-2)
- [ ] [2-1-3  开发特征时候做如何做数据探索，怎样选择有用的特征？](#2-1-3)
- [ ] [2-1-4  你是如何做数据清洗的？举例说明](#2-1-4)
- [ ] [2-1-5  如何发现数据中的异常值，你是如何处理？](#2-1-5)
- [ ] [2-1-6  缺失值如何处理？](#2-1-6)
- [ ] [2-1-7  对于数值类型数据，你会怎样处理？为什么要做归一化？归一化有哪些方法？离散些方法，离散化和归一化有哪些优缺点？](#2-1-7)
- [ ] [2-1-8  标准化和归一化异同？](#2-1-8)
- [ ] [2-1-9  你是如何处理CTR类特征？](#2-1-9)
- [ ] [2-1-10  讲解贝叶斯平滑原理？以及如何训练得到平滑参数](#2-1-10)
- [ ] [2-1-11  类别型数据你是如何处理的？比如游戏品类，地域，设备](#2-1-11)
- [ ] [2-1-12  序号编码、one-hot编码、二进制编码都是什么？适合怎样的类别型数据？](#2-1-12)
- [ ] [2-1-13  时间类型数据你的处理方法是什么？原因？](#2-1-13)
- [ ] [2-1-14  你怎样理解组合特征？举个例子，并说明它和单特征有啥区别](#2-1-14)
- [ ] [2-1-15  如何处理高维组合特征？比如用户ID和内容ID？](#2-1-15)
- [ ] [2-1-16  如何理解笛卡尔积、外积、内积？](#2-1-16)
- [ ] [2-1-17  文本数据你会如何处理？](#2-1-17)
- [ ] [2-1-18  文本特征表示有哪些模型？他们的优缺点都是什么？](#2-1-18)

**词袋模型**（Bag of Words）：每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一位代表一个单词，该维对应的权重则反映这个词在原文章中的重要程度，常用TF-IDF来计算权重。

缺点：无法识别出两个不同的词或者词组具有相同的主题。

**N-gram模型**：将连续出现的n个词（n<=N）组成的词组也作为一个单独的特征放到向量表示中。

缺点：无法识别出两个不同的词或者词组具有相同的主题。

**主题模型**（Topic Model）：是一种特殊的概率图模型

**词嵌入模型**（Word Embedding）：将词向量化；核心思想是将每个词映射到低维空间（K=50~300维）上的一个稠密向量。K维空间的每一维也可以看作一个隐含的主题



- [ ] [2-1-19  讲解TFF原理，它有什么优点和缺点？针对它的缺点，你有什么优化思路？](#2-1-19)

$$
{\rm TF-IDF}(t,d) = {\rm TF}(t,d) \times {\rm IDF}(t)
$$

其中 ${\rm TF}(t,d)$ 为单词t在文档d中出现的频率，${\rm IDF}(t)$ 是逆文档频率，用来衡量单词t对表达语义所起的重要性
$$
{\rm IDF}(t) = {\rm log} \frac{文章总数}{包含单词t的文章总数+1}
$$
优点：



- [ ] [2-1-20  N-gram算法是什么？有什么优缺点？](#2-1-20)
- [ ] [2-1-21  讲解一下word2vec工作原理？损失函数是什么？](#2-1-21)
- [ ] [2-1-22  讲解一下LDA模型原理和训练过程？](#2-1-22)
- [ ] [2-1-23  Word2vec和LDA两个模型有什么区别和联系？](#2-1-23)
- [ ] [2-1-24  Skin-gram和cbow有何异同？](#2-1-24)

不同点：skip-gram使用中心词预测上下文；CBOW用上下文预测中心词

相同点：都是根据word共现的频率建模word的相似度



- [ ] [2-1-25  图像数据如何处理？有哪些常用的图像特征提取方法](#2-1-25)
- [ ] [2-1-26  你是怎样做特征选择的？卡方检验、信息值（IV）、VOE都是如何计算？](#2-1-26)
- [ ] [2-1-27  计算特征之间的相关性方法有哪些？有什么优缺点](#2-1-27)



### **基础算法原理和推导**

#### **KNN**

- [ ] [2-2-1  Knn建模流程是怎样的？](#2-2-1)

- [ ] [2-2-2  Knn优缺点是什么？](#2-2-2)

- [ ] [2-2-3  Knn适合什么样的场景和数据类型？](#2-2-3)

- [ ] [2-2-4  常用的距离衡量公式都有哪些？具体说明它们的计算流程，以及使用场景？](#2-2-4)

- [ ] [2-2-5  超参数K值过大或者过小对结果有什么影响，你是如何选择K值？](#2-2-5)

- [ ] [2-2-6  介绍一下Kd树？如何建树，以及如何搜索最近节点？](#2-2-6)

  ·

#### **支持向量机**

- [ ] [2-3-1  简单讲解SVM模型原理？](#2-3-1)
- [ ] [2-3-2  SVM为什么会对缺失值敏感？实际应用时候你是如何处理？](#2-3-2)
- [ ] [2-3-3  SVM为什么可以分类非线性问题？](#2-3-3)
- [ ] [2-3-4  常用的核函数有哪些？你是如何选择不同的核函数的？](#2-3-4)

**高斯核函数** RBF kernel
$$
K(x,z) = exp(-\frac{1}{2} \ ||x - z ||_2 ) = \phi(x) \cdot \phi(z)
$$

$$
f(x) = {\rm sign} ()
$$



**Sigmod核函数**



**字符串核函数**

​	在一些结构化的数据，如字符串序列中，序列长度未知，难以设计 $\phi(x)$。$K(x,z)$ 类似相似度，我们使用SVM时只要定K的形式，去计算序列的相似度。





- [ ] [2-3-5  RBF核函数一定线性可分么？为什么](#2-3-5)
- [ ] [2-3-6  SVM属于线性模型还是非线性模型？为什么？](#2-3-6)
- [ ] [2-3-7  训练误差为0的SVM分类器一定存在吗？说明原因？](#2-3-7)



#### **朴素贝叶斯模型**

- [ ] [2-4-1  讲解贝叶斯定理？](2-4-1)
- [ ] [2-4-2  什么是条件概率、边缘概率、联合概率？](#2-4-2)
- [ ] [2-4-3  后验概率最大化的含义是什么？](#2-4-3)
- [ ] [2-4-4  朴素贝叶斯模型如何学习的？训练过程是怎样？](#2-4-4)
- [ ] [2-4-5  你如何理解生成模型和判别模型？](#2-4-5)
- [ ] [2-4-6  朴素贝叶斯模型“朴素”体现在哪里？存在什么问题？有哪些优化方向？](#2-4-6)
- [ ] [2-4-7  什么是贝叶斯网络？它能解决什么问题？](#2-4-7)
- [ ] [2-4-8  为什么说朴素贝叶斯也是线性模型而不是非线性模型呢？](#2-4-8)



#### **线性回归**

- [ ] [2-5-1  线性回归的基本思想是？](#2-5-1)
- [ ] [2-5-2  什么是“广义线性模型”？](#2-5-2)
- [ ] [2-5-3  线性回归常用的损失函数有哪些？优化算法有哪些？](#2-5-3)
- [ ] [2-5-4  线性回归适用什么类型的问题？有哪些优缺点？](#2-5-4)
- [ ] [2-5-5  请用最小二乘法推倒参数更新公式？](#2-5-5)



#### **逻辑回归**

- [ ] [2-6-1  逻辑回归相比于线性回归有什么异同？](#2-6-1)

**不同点**：

逻辑回归处理的是分类问题，线性回归处理的是回归问题；

逻辑回归中认为y是因变量，即逻辑回归的因变量是离散的，线性回归的因变量是连续的。



**相同点：**

二者都使用了极大似然估计来对训练样本进行建模

求解超参数过程中，都可以使用梯度下降的方法



**联系**：

如果把一个事件的几率（odds）定义为该事件发生的概率与不发生概率的比值 $\frac{p}{1-p}$ ，那么逻辑回归可以看做是对于"y=1|x"这一事件的对数几率的线性回归
$$
{\rm log} \frac{p}{1-p} = \theta^{T}x ，其中\ p  = P(y=1|x)
$$


- [ ] [2-6-2  逻辑回归和广义线性模型有何关系？](#2-6-2)

可以看做广义线性模型在因变量y服从二元分布时的一个特殊情况


- [ ] [2-6-3  逻辑回归如何处理多标签分类？](#2-6-3)

如果一个样本只对应于一个标签（多分类问题）：
假设每个样本属于不同标签的概率服从几何分布，使用softmax regression进行分类：
$$
h_\theta =
 \left[
 \begin{matrix}
   p(y=1|x;\theta)\\
   p(y=2|x;\theta) \\
   \vdots \\
   p(y=1|x;\theta)
  \end{matrix}
  \right] 
  = 
  \frac{1}{\sum_{j=1}^{k} e^{\theta^T x}}
  
  \left[
 \begin{matrix}
   e^{\theta_1^T x}\\
   e^{\theta_2^T x} \\
   \vdots \\
   e^{\theta_k^T x}
  \end{matrix}
  \right] 
  
  \tag{3}
$$
其中 $\theta_1,\theta_2 \dots,\theta_k \in \mathbb{R}^n​$

如果存在样本可能属于多个标签的情况时，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类。



- [ ] [2-6-4  为什么逻辑回归需要进行归一化或者取对数？](#2-6-4)

- [ ] [2-6-5  为什么逻辑回归把特征离散化之后效果会提升？](#2-6-5)

- [ ] [2-6-6  类别不平衡问题你是如何处理的？什么是过采样，什么是欠采样？举例](#2-6-6)

- [ ] [2-6-7  讲解L1和L2正则，它们都有什么作用，解释为什么L1比L2更容易产生稀疏解；对于存在线性相关的一组特征，L1正则如何选择特征？](#2-6-7)

L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0

从解空间的形状来看：

L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。

对于存在线性相关的一组特征，L1正则会使得部分参数为0



- [ ] [2-6-8  使用交叉熵作为损失函数，梯度下降作为优化方法，推倒参数更新公式](#2-6-8)
- [ ] [2-6-9  代码写出训练函数](#2-6-9)



#### **FM模型**

- [ ] [2-7-1  FM模型与逻辑回归相比有什么优缺点？](#2-7-1)
- [ ] [2-7-2  为什么FM模型计算复杂度时O(kn)？](#2-7-2)
- [ ] [2-7-3  介绍FFM场感知分解机器（Field-aware Factorization Machine），说说与FM异同？](#2-7-3)
- [ ] [2-7-4  使用FM进行模型训练时候，有哪些核心参数对模型效果影响大？](#2-7-4)
- [ ] [2-7-5  如何从神经网络的视角看待FM模型？](#2-7-5)



#### **决策树**

- [x] [2-8-1  讲解完成的决策树的建树过程](#2-8-1)

自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。



- [x] [2-8-2  你是如何理解熵？从数学原理上解释熵公式可以作为信息不确定性的度量？](#2-8-2)

熵（entropy）是表示随机变量不确定性的度量， $X$ 是一个取有限个值的离散随机变量，其概率分布为
$$
P(X = x_i) = p_i, \ i=1,2,\cdots,n
$$
则随机变量 $X$ 的熵定义为
$$
H(X) = \sum_{i=1}^{n} p_i {\rm log } \ p_i 
$$
熵越大，随机变量的不确定性就越大。



而熵其实表示的是一个系统的平均信息量。**自信息量**是用来描述某一条信息的大小
$$
I = - {\rm log} \ p_i
$$
通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。而通常我们衡量整个系统的信息量，系统存在多个事件 $X=\{x_1,\cdots,x_n\}$ ，每个事件的概率分布$P=\{p_1,\cdots,p_n\}$ ，**熵是整个系统的平均信息量** 。



- [ ] [2-8-3  联合熵、条件熵、KL散度、信息增益、信息增益比、gini系数都是什么？如何计算？](#2-8-3)

**联合熵**：

**条件熵**：某个特征A对于数据集D的经验条件熵 $H(D|A)$ 为
$$
H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \\ = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \lgroup \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} {\rm log } \frac{|D_{ik}|}{|D_i|} \rgroup
$$
**信息增益**： $g(D,A)$ 定义为数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
$$
g(D,A) = H(D) - H(D|A)g(D,A) = H(D) - H(D|A)
$$
**信息增益比**：特征A对于数据集D 的信息增益比定义为
$$
g_R(D|A) = \frac{g(D|A)}{H_A(D)}
$$
其中 
$$
H_A{(D)} = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} {\rm log } \frac{|D_i|}{|D|}
$$
为数据集D关于A的取值熵；n为特征A在D上的取值数目；



**Gini系数**：描述数据的纯度。数据集D的Gini系数为
$$
{\rm Gini}(D) = 1 - \sum_{k=1}^{K
}(\frac{|C_k|}{|D|})^2
$$
其中 $C_k$是 D中第k类的样本子集，K是类的个数。



- [x] [2-8-4  常用的决策树有哪些？ID3、C4.5、CART有啥异同？](#2-8-4)

| 不同点   | ID3                  | C4.5               | CART                 |
| -------- | -------------------- | ------------------ | -------------------- |
| 用途     | 分类                 | 分类               | 分类、回归           |
| 输入取值 | 离散                 | 离散、连续         | 离散、连续           |
| 树结构   | 多叉树               | 多叉树             | 二叉树               |
|          | 特征在层级间不复用   | 特征在层级间不复用 | 每个特征可被重复利用 |
|          | 对样本特征缺失值敏感 |                    |                      |



**ID3	最大信息增益**

信息增益 $g(D,A)$ 定义为数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
$$
g(D,A) = H(D) - H(D|A)
$$
选择 $g(D,A)$  最大的特征，所有样本根据此特征，划分到不同的节点上。在经验熵不为0的节点中继续生长。ID3算法只有树的生成，容易产生过拟合。



**C4.5	最大信息增益比**

因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，是一个信息增益比来选择最优划分属性。



**CART	基尼指数**

Gini用来描述数据的纯度，越小纯度越高。CART在每一次迭代中选择划分后**基尼指数最小**的特征及其对应的切分点进行分类。CART是一颗二叉树，每次将数据按特征A的区分分成两份，分别进入左右子树。



- [ ] [2-8-5  决策树如何防止过拟合？前剪枝和后剪枝过程是怎样的？剪枝条件都是什么](#2-8-5)

通过剪枝防止过拟合。

**预剪枝**是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；

**后剪枝**则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶子节点进行考察，若该节点对应的子树替换成叶子结点能带来泛化性能提升，则将该子树替换为叶子节点。





#### **随机森林（RF）**

- [ ] [2-9-1  介绍RF原理和思想](#2-9-1)
- [ ] [2-9-2  RF是如何处理缺失值？](#2-9-2)
- [ ] [2-9-3  RF如何衡量特征重要度？](#2-9-3)
- [ ] [2-9-4  RF“随机”主要体现在哪里？](#2-9-4)
- [ ] [2-9-5  RF有哪些优点和局限性？](#2-9-5)
- [ ] [2-9-6  为什么多个弱分类器组合效果会比单个要好？如何组合弱分类器可以获得更好的结果？原因是什么？](#2-9-6)
- [ ] [2-9-7  Bagging的思想是什么？它是降低偏差还是方差，为什么？](#2-9-7)
- [ ] [2-9-8  可否将RF的基分类模型由决策树改成线性模型或者knn？为什么？](#2-9-8)



#### **GBDT**

- [ ] [2-10-1  梯度提升和梯度下降有什么区别和联系？](#2-10-1)
- [ ] [2-10-2  你是如何理解Boosting和Bagging？他们有什么异同？](#2-10-2)
- [ ] [2-10-3  讲解GBDT的训练过程？](#2-10-3)
- [ ] [2-10-4  你觉得GBDT训练过程中哪些环节可以平行提升训练效率？](#2-10-4)
- [ ] [2-10-5  GBDT的优点和局限性有哪些？](#2-10-5)
- [ ] [2-10-6  GBDT是否对异常值敏感，为什么？](#2-10-6)
- [ ] [2-10-7  如何防止GBDT过拟合？](#2-10-7)
- [ ] [2-10-8  在训练过程中哪些参数对模型效果影响比较大？这些参数造成影响是什么？](#2-10-8)



#### **k-means**

- [ ] [2-11-1  简述kmeans建模过程？](#2-11-1)
- [ ] [2-11-2  Kmeans损失函数是如何定义？](#2-11-2)
- [ ] [2-11-3  你是如何选择初始类族的中心点？](#2-11-3)
- [ ] [2-11-4  如何提升kmeans效率？](#2-11-4)
- [ ] [2-11-5  常用的距离衡量方法有哪些？他们都适用什么类型问题？](#2-11-5)
- [ ] [2-11-6  Kmeans对异常值是否敏感？为什么？](#2-11-6)
- [ ] [2-11-7  如何评估聚类效果？](#2-11-7)
- [ ] [2-11-8  超参数类的个数k如何选取？](#2-11-8)
- [ ] [2-11-9  Kmeans有哪些优缺点？是否有了解过改进的模型，举例说明？](#2-11-9)
- [ ] [2-11-10  试试证明kmeans算法的收敛性](#2-11-10)
- [ ] [2-11-11  除了kmeans聚类算法之外，你还了解哪些聚类算法？简要说明原理](#2-11-11)



#### **PCA降维**

- [ ] [2-12-1  为什么要对数据进行降维？它能解决什么问题？](#2-12-1)
- [ ] [2-12-2  你是如何理解维度灾难？](#2-12-1)
- [ ] [2-12-3  PCA主成分分析思想是什么？](#2-12-1)
- [ ] [2-12-4  如何定义主成分？](#2-12-1)
- [ ] [2-12-5  如何设计目标函数使得降维达到提取主成分的目的？](#2-12-1)
- [ ] [2-12-6  PCA有哪些局限性？如何优化](#2-12-1)
- [ ] [2-12-7  线性判别分析和主成分分析在原理上有何异同？在目标函数上有何区别和联系？](#2-12-1)



## **3、 深度学习**

#### **DNN**

- [ ] [3-1-1  描述一下神经网络？推倒反向传播公式？](#3-1-1)
- [ ] [3-1-2  讲解一下dropout原理？](#3-1-2)
- [ ] [3-1-3  梯度消失和梯度膨胀的原因是什么？有什么方法可以缓解？](#3-1-3)
- [ ] [3-1-4  什么时候该用浅层神经网络，什么时候该选择深层网络](#3-1-4)
- [ ] [3-1-5  Sigmoid、Relu、Tanh激活函数都有哪些优缺点？](#3-1-5)

**Sigmoid**
$$
f(x) = \frac{1}{1+ exp(-x)} \\
f'(x) = f(x)(1-f(x))
$$
优点：

缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题





**Tanh**
$$
f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\

f'(x) = 1 - (f(x))^2
$$
优点：

缺点：（1）需要计算指数，速度慢（2）会产生梯度消失问题



**Relu**
$$
f(x) = max(x,0) \\
\begin{equation}
f'(x)=\left\{
\begin{aligned}
    1,x>0 \\
    0,x\leq0 \\
\end{aligned}
\right.
\end{equation}
$$
优点：（1）从计算的角度上，sigmoid和tanh都需要计算指数，复杂度高，而ReLU只需要一个阈值就可以得到激活值

（2）ReLU的非饱和性可以有效解决梯度消失的问题，提供相对宽的激活边界

（3）ReLU的单侧抑制提供了网络的稀疏表达能力（防止过拟合）

缺点：（1）训练过程中会导致神经元死亡的问题

缺点：（1）训练过程中会导致神经元死亡的问题



**Leaky ReLU**
$$
\begin{equation}
f(x)=\left\{
\begin{aligned}
    x,x>0 \\
    ax,x\leq0 \\
\end{aligned}
\right.
\end{equation}
\\

f'(x)=\left\{
\begin{aligned}
    1,x>0 \\
    a,x\leq0 \\
\end{aligned}
\right.
$$
优点：实现单侧抑制，又保留了部分附体度信息以致不完全消失

缺点：a值需要人工选择





- [ ] [3-1-6  写出常用激活函数的导数](#3-1-6)

见上

- [ ] [3-1-7  训练模型的时候，是否可以把网络参数全部初始化为0？为什么](#3-1-7)

不可以；参数全部为0时，网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，会使得更新后的参数仍然保持完全相同。从而使得模型无法训练。



- [ ] [3-1-8  Batchsize大小会如何影响收敛速度？](#3-1-8)



#### **CNN**

- [ ] [3-2-1  简述CNN的工作原理？](#3-2-1)

CNN利用了图像的三个性质：

（1）图像的pattern通常比整张图像小

（2）通用的patterns会出现在图像的不同区域

（3）对图像进行子采样并不影响图像的识别

CNN通过卷积层+pooling层不断堆积，从小的pattern开始不断识别到大的pattern，从而识别整张图像。

> CNN适合处理什么问题

具有以上三个特性的问题

- [ ] [3-2-2  卷积核是什么？选择大卷积核和小卷积核有什么影响？](#3-2-2)
- [ ] [3-2-3  你在实际应用中如何设计卷积核？](#3-2-3)
- [ ] [3-2-4  为什么CNN具有平移不变性？](#3-2-4)

参数共享



- [ ] [3-2-5  Pooling操作是什么？有几种？作用是什么？](#3-2-5)
- [ ] [3-2-6  为什么CNN需要pooling操作？](#3-2-6)
- [ ] [3-2-7  什么是batchnormalization？它的原理是什么？在CNN中如何使用？](#3-2-7)
- [ ] [3-2-8  卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性以其作用？](#3-2-8)

**稀疏交互**：每个神经元的只跟上一层的某些神经元连接（vs DNN全连接），用到较少参数

**参数共享**：同一层的不同神经元之间共享部分权重，用到比原来更少的参数



- [ ] [3-2-9  你是如何理解fine-tune？有什么技巧](#3-2-9)

- [ ] [3-2-10  怎么观察CNN每个神经元学到了什么](#3-2-10)

假设第k个filter是一个11 x 11 的矩阵（一个神经元），可以用以下系数来表示第k个filter被激活的程度
$$
a^k = \sum_{i=1}^{11} \sum_{i=1}^{11} a_{ij}^k
$$
并通过梯度上升找到使 $a^k$ 最大的x，该x表示的图像表示该filter对应的检测纹路。
$$
x^* = \mathop{arg \ \rm max}\limits_{x}  \ a^k
$$


#### **RNN**

- [ ] [3-3-1  简述RNN模型原理，说说RNN适合解决什么类型问题？为什么](#3-3-1)

- [ ] [3-3-2  RNN和DNN有何异同？](#3-3-2)

- [ ] [3-3-3  RNN为什么有记忆功能？](#3-3-3)

- [ ] [3-3-4  长短期记忆网络LSTM是如何实现长短期记忆功能的？](#3-3-4)

- [ ] [3-3-5  长短期记忆网络LSTM各模块都使用什么激活函数，可以使用其他激活函数么？](#3-3-5)

- [ ] [3-3-6  GRU和LSTM有何异同](#3-3-6)

- [ ] [3-3-7  什么是Seq2Seq模型？该模型能解决什么类型问题？](#3-3-7)

- [ ] [3-3-8  注意力机制是什么？Seq2Seq模型引入注意力机制主要解决什么问题？](#3-3-8)




## 4、 基础工具

#### **Spark**

- [ ] [4-1-1  什么是宽依赖，什么是窄依赖？哪些算子是宽依赖，哪些是窄依赖？](#4-1-1)
- [ ] [4-1-2  Transformation和action算子有什么区别？举例说明](#4-1-2)
- [ ] [4-1-3  讲解sparkshuffle原理和特性？shuffle write 和 huffleread过程做些什么？](#4-1-1)
- [ ] [4-1-4  哪些spark算子会有shuffle？](#4-1-1)
- [ ] [4-1-5  讲解sparkschedule（任务调度）？](#4-1-1)
- [ ] [4-1-6  Sparkstage是如何划分的？](#4-1-1)
- [ ] [4-1-7  Sparkcache一定能提升计算性能么？说明原因？](#4-1-1)
- [ ] [4-1-8  Cache和persist有什么区别和联系？](#4-1-1)
- [ ] [4-1-9  RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷？](#4-1-1)
- [ ] [4-1-10  当GC时间占比很大可能的原因有哪些？对应的优化方法是？](#4-1-1)
- [ ] [4-1-11  park中repartition和coalesce异同？coalesce什么时候效果更高，为什么](#4-1-1)
- [ ] [4-1-12  Groupbykey和reducebykey哪个性能更高，为什么？](#4-1-1)
- [ ] [4-1-13  你是如何理解caseclass的？](#4-1-1)
- [ ] [4-1-14  Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候用class](#4-1-1)
- [ ] [4-1-15  Scala 语法中to 和 until有啥区别](#4-1-1)
- [ ] [4-1-16  讲解Scala伴生对象和伴生类](#4-1-1)





#### **Xgboost**

- [ ] [4-2-1  你选择使用xgboost的原因是什么？](#4-2-1)
- [ ] [4-2-2  Xgboost和GBDT有什么异同？](#4-2-2)
- [ ] [4-2-3  为什么xgboost训练会那么快，主要优化点事什么？](#4-2-3)
- [ ] [4-2-4  Xgboost是如何处理缺失值的？](#4-2-4)
- [ ] [4-2-5  Xgboost和lightGBM有哪些异同？](#4-2-5)
- [ ] [4-2-6  Xgboost为什么要使用泰勒展开式，解决什么问题？](#4-2-6)
- [ ] [4-2-7  Xgboost是如何寻找最优特征的？](#4-2-7)



#### **Tensorflow**

- [ ] [4-3-1  使用tensorflow实现逻辑回归，并介绍其计算图](#4-3-1)
- [ ] [4-3-2  sparse_softmax_cross_entropy_with_logits和softmax_cross_entropy_with_logits有何异同？](#4-3-2)
- [ ] [4-3-3  使用tensorflow过程中，常见调试哪些参数？举例说明](#4-3-3)
- [ ] [4-3-4  Tensorflow梯度更新是同步还是异步，有什么好处？](#4-3-4)
- [ ] [4-3-5  讲解一下TFRecords](#4-3-5)
- [ ] [4-3-6  tensorflow如何使用如何实现超大文件训练？](#4-3-6)
- [ ] [4-3-7  如何读取或者加载图片数据？](#4-3-7)



## **5、推荐系统**

- [ ] [5-1-1  你是如何选择正负样本？如何处理样本不均衡的情况？](#5-1-1)

样本不均衡处理：

1）上采样和子采样；2）修改权重（修改损失函数）；3）集成方法：bagging，类似随机森林、自助采样；4）多任务联合学习；



- [ ] [5-1-2  如何设计推荐场景的特征体系？举例说明](#5-1-1)
- [ ] [5-1-3  你是如何建立用户模型来理解用户，获取用户兴趣的？](#5-1-1)
- [ ] [5-1-4  你是如何选择适合该场景的推荐模型？讲讲你的思考过程](#5-1-1)
- [ ] [5-1-5  你是如何理解当前流行的召回->粗排->精排的推荐架构？这种架构有什么优缺点？什么场景适用使用，什么场景不适合？](#5-1-1)
- [ ] [5-1-6  如何解决热度穿透的问题？（因为item热度非常高，导致ctr类特征主导排序，缺少个性化的情况）](#5-1-1)
- [ ] [5-1-7  用户冷启动你是如何处理的？](#5-1-1)
- [ ] [5-1-8  新内容你是如何处理的？](#5-1-1)
- [ ] [5-1-9  你们使用的召回算法有哪些？如何能保证足够的召回率？](#5-1-1)
- [ ] [5-1-10  实时数据和离线数据如何融合？工程上是怎样实现？如何避免实时数据置信度不高带来的偏差问题？](#5-1-1)
- [ ] [5-1-11  你们是如何平衡不同优化目标的问题？比如：时长、互动等](#5-1-1)
- [ ] [5-1-12  不同类型内容推荐时候，如何平衡不同类型内容，比如图文、视频；或者不同分类](#5-1-1)
- [ ] [5-1-13  如何保证线上线下数据一致性？工程上是如何实现？](#5-1-1)
- [ ] [5-1-14  离线训练效果好，但是上线效果不明显或在变差可能是什么问题？如何解决？](#5-1-1)
- [ ] [5-1-15  在实际业务中，出现badcase,你是如何快速反查问题的？举例说明](#5-1-1)
- [ ] [5-1-16  使用ctr预估的方式来做精排，会不会出现相似内容大量聚集？原因是什么？你是如何解决的？](#5-1-1)
- [ ] [5-1-17  你了解有多少种相关推荐算法？各有什么优缺点](#5-1-1)



|               | 优点                     | 缺点                       | 适用场景                                       |
| ------------- | ------------------------ | -------------------------- | ---------------------------------------------- |
| item-based CF | 注重个性化，发掘长尾商品 | 推荐结果过于热门，需要惩罚 | item更新频率低的（如购物网站、图书、电影网站） |
| user-based CF | 社交网络                 |                            | item更新频率高（新闻）                         |
|               |                          |                            |                                                |

l

- [ ] [5-1-18  深度学习可以应用到推荐问题上解决哪些问题？为什么比传统机器学习要好？](#5-1-1)



# **二、数学相关**

## **6、 概率论和统计学**

- [ ] [6-1-1  说说你是怎样理解信息熵的？](#6-1-1)
- [ ] [6-1-2   能否从数据原理熵解析信息熵可以表示随机变量的不确定性？](#6-1-2)
- [ ] [6-1-3  怎样的模型是最大熵模型？它有什么优点](#6-1-3)
- [ ] [6-1-4  什么是Beta分布？它与二项分布有什么关系？](#6-1-4)
- [ ] [6-1-5   什么是泊松分布？它与二项分布有什么关系？](#6-1-5)
- [ ] [6-1-6  什么是t分布？他与正态分布有什么关系？](#6-1-6)
- [ ] [6-1-7   什么是多项式分布？具体说明？](#6-1-7)
- [ ] [6-1-8   参数估计有哪些方法？](#6-1-8)
- [ ] [6-1-9  点估计和区间估计都是什么？](#6-1-9)
- [ ] [6-1-10  讲解一下极大似然估计，以及适用场景？](#6-1-10)

**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。**

那么我们就知道了极大似然估计的核心关键就是对于一些情况，样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参



## 7、 最优化问题

- [ ] [7-1-1  什么是梯度？](#7-1-1)
- [ ] [7-1-2  梯度下降找到的一定是下降最快的方法？](#7-1-1)
- [ ] [7-1-3  牛顿法和梯度法有什么区别？](#7-1-1)
- [ ] [7-1-4  什么是拟牛顿法？](#7-1-1)
- [ ] [7-1-5  讲解什么是拉格朗日乘子法、对偶问题、kkt条件?](#7-1-1)
- [ ] [7-1-6  是否所有的优化问题都可以转化为对偶问题？](#7-1-1)
- [ ] [7-1-7  讲解SMO（SequentialMinimalOptimization）算法基本思想？](#7-1-1)
- [ ] [7-1-8  为什么深度学习不用二阶优化？](#7-1-1)
- [ ] [7-1-9  讲解SGD，ftrl、Adagrad，Adadelta，Adam，Adamax，Nadam优化算法以及他们的联系和优缺点](#7-1-1)
- [ ] [7-1-10 为什么batch size大，训练速度快or为什么mini-batch比SGD快？](#7-1-10)

（1）从矩阵计算角度

mini-batch不同的输入可以拼成一个矩阵，和W计算矩阵相乘可以并行地计算，比SGD一个一个计算快。对于GPU，矩阵相乘可以并行地处理，速度快。

- [ ] [7-1-11  7-1-11  为什么不把batch size设得特别大](7-1-11)

（1）GPU内存限制

（2）性能差；容易卡在局部极小值  





